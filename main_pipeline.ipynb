{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook will allow you to run a model on a single prepared dataset. \n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import ffmpeg\n",
    "import pdb\n",
    "from functions import create_pytorch_dataset\n",
    "from functions import get_window_metrics\n",
    "from functions import get_frame_metrics\n",
    "from functions import animate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets load the H%PY dataset into a pytorch dataset class.Please see \n",
    "# dataset_creator on how to generate the H5PY file. \n",
    "\n",
    "# Name of the H5PY dataset \n",
    "dset = \"Edits/ZED_Depth\" #where the orginal data is stored \n",
    "name = \"ZED_Depth_Filled\" # name of the h5py file\n",
    "path = \"H5Data\\Data_set-{}-imgdim64x64.h5\".format(name) # location of the h5py file\n",
    "# this will also window the data at a set size, and with the set stride \n",
    "\n",
    "window_len = 8\n",
    "stride = 1\n",
    "fair_comparison = False\n",
    "\n",
    "Test_Dataset, test_dataloader, Train_Dataset, train_dataloader = create_pytorch_dataset(name, dset, path, window_len, fair_comparison, stride)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now that we have our datasets prepared, lets write our model \n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # first layer\n",
    "        self.ec1 = nn.Conv3d(1, 16, (5, 3, 3), stride=1, padding=(2, 1, 1),)\n",
    "        self.em1 = nn.MaxPool3d((1, 2, 2), return_indices=True)\n",
    "        #self.ed1 = nn.Dropout3d(p=0.25)\n",
    "        # second layer\n",
    "        self.ec2 = nn.Conv3d(16, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.em2 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "        #self.ed2 = nn.Dropout3d(p=0.25)\n",
    "        # third layer\n",
    "        self.ec3 = nn.Conv3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.em3 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "        # encoding done, time to decode\n",
    "        self.dc1 = nn.ConvTranspose3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm1 = nn.MaxUnpool3d((2, 2, 2))\n",
    "        # inverse of 2nd Conv\n",
    "        self.dc2 = nn.ConvTranspose3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm2 = nn.MaxUnpool3d((2, 2, 2))\n",
    "        # inverse of 1st Conv\n",
    "        self.dc3 = nn.ConvTranspose3d(8, 16, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm3 = nn.MaxUnpool3d((1, 2, 2))\n",
    "        # final inverse\n",
    "        self.dc4 = nn.ConvTranspose3d(16, 1, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # *** start of encoder\n",
    "        x = x.permute(1, 0, 2, 3, 4)  # reorder to have correct dimensions\n",
    "        \n",
    "        # (batch_size, chanels, depth, width, height)\n",
    "        _ec1 = F.relu(self.ec1(x))\n",
    "        _em1, i1 = self.em1(_ec1)\n",
    "\n",
    "\n",
    "        #_em1 = self.ed1(_em1) # dropout layer\n",
    "        # second layer \n",
    "        _ec2 = F.relu(self.ec2(_em1))\n",
    "        _em2, i2 = self.em2(_ec2)\n",
    "        \n",
    "        #_em2 = self.ed2(_em2) # dropout layer\n",
    "        # third layer\n",
    "        _ec3 = F.relu(self.ec3(_em2))\n",
    "        _em3, i3 = self.em3(_ec3)\n",
    "     \n",
    "        # print(\"====== Encoding Done =========\")\n",
    "        # *** encoding done, time to decode\n",
    "        _dc1 = F.relu(self.dc1(_em3))\n",
    "        _dm1 = self.dm1(_dc1, i3, output_size=_em2.size())\n",
    "        \n",
    "        # second layer\n",
    "        _dc2 = F.relu(self.dc2(_dm1))\n",
    "        _dm2 = self.dm2(_dc2, i2)\n",
    "        \n",
    "        # third layer\n",
    "        _dc3 = F.relu(self.dc3(_dm2))\n",
    "        _dm3 = self.dm3(_dc3, i1)\n",
    "        \n",
    "        re_x = torch.tanh(self.dc4(_dm3))\n",
    "        '''\n",
    "        print(x.shape)\n",
    "        print(_ec1.shape)\n",
    "        print(_em1.shape)\n",
    "        print(_ec2.shape)\n",
    "        print(_em2.shape)\n",
    "        print(_ec3.shape)\n",
    "        print(_em3.shape)\n",
    "        print(_dc1.shape)\n",
    "        print(_dm1.shape)\n",
    "        print(_dc2.shape)\n",
    "        print(_dm2.shape)\n",
    "        print(_dc3.shape)\n",
    "        print(_dm3.shape)\n",
    "        print(re_x.shape)\n",
    "        '''\n",
    "        \n",
    "        return re_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets train our model\n",
    "\n",
    "# prepare for GPU training \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# and lets set the hyperparameters! \n",
    "\n",
    "dropout = 0.25\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 20\n",
    "chunk_size = 128\n",
    "forward_chunk = 8 \n",
    "forward_chunk_size = 8 # this is smaller due to memory constrains \n",
    "\n",
    "# select which model - you could load your own or put it in the function above \n",
    "model = Autoencoder().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(filepath):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        val_loss = 0\n",
    "        for i, (sample, labels) in enumerate(train_dataloader):\n",
    "            # ===================forward=====================\n",
    "            sample = sample.to(device, dtype=torch.float)\n",
    "            # split sample into smaller sizes due to GPU memory constraints\n",
    "            chunks = torch.split(sample, chunk_size, dim=1)\n",
    "            for chunk in chunks:\n",
    "                output = model(chunk)\n",
    "                output = output.to(device).permute(1, 0, 2, 3, 4)\n",
    "                model.zero_grad()\n",
    "                loss = loss_fn(output, chunk)\n",
    "                # ===================backward====================\n",
    "                # Getting gradients w.r.t. parameters\n",
    "                loss.backward()\n",
    "                # Updating parameters\n",
    "                optimizer.step()\n",
    "                # Clear gradients w.r.t. parameters\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.empty_cache()\n",
    "        # ===================log========================\n",
    "        print(\"epoch [{}/{}], loss:{:.4f}\".format(epoch + 1, num_epochs, loss.item()))\n",
    "        torch.save(model.state_dict(), filepath) # save the model each epoch at location filepath\n",
    "        \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "'''\n",
    "model = Autoencoder().to(device)\n",
    "print(model)\n",
    "#make_dot(chunk, params=dict(model.named_parameters()))\n",
    "make_dot(chunk, params=dict(list(model.named_parameters()))).render(\"test_graphic\", format=\"png\")\n",
    "\n",
    "for j, (sample, labels) in enumerate(test_dataloader):\n",
    "    print(j)\n",
    "    # foward pass to get output\n",
    "    torch.cuda.empty_cache()\n",
    "    sample = sample.to(device, dtype=torch.float)\n",
    "    chunks = torch.split(sample, forward_chunk, dim=1)\n",
    "    recon_vid = []\n",
    "    for chunk in chunks:\n",
    "        output = model(chunk)\n",
    "        vis_graph = make_dot(output, params=dict(model.named_parameters())).render(\"test_graphic\", format=\"png\")\n",
    "        break\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def foward_pass(path):\n",
    "    model.load_state_dict(torch.load(path)) # load a saved model \n",
    "    model.eval()\n",
    "    frame_stats = [] \n",
    "    window_stats = [] \n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"foward pass occuring\")\n",
    "        # just forward pass of model on test dataset\n",
    "        for j, (sample, labels) in enumerate(test_dataloader):\n",
    "            print(j)\n",
    "            # foward pass to get output\n",
    "            torch.cuda.empty_cache()\n",
    "            sample = sample.to(device, dtype=torch.float)\n",
    "            chunks = torch.split(sample, forward_chunk, dim=1)\n",
    "            recon_vid = []\n",
    "            for chunk in chunks:\n",
    "                output = model(chunk)\n",
    "                output = output.to(device).permute(1, 0, 2, 3, 4)\n",
    "                recon_vid.append(output)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            output = torch.cat(recon_vid, dim=1)\n",
    "            # convert tensors to numpy arrays for easy manipluations\n",
    "            sample = sample.data.cpu().numpy()\n",
    "            output = output.data.cpu().numpy()\n",
    "            labels = labels.data.cpu().numpy()\n",
    "\n",
    "            frame_mean, frame_std, frame_labels = get_frame_metrics(output, sample, labels, window_len)\n",
    "            mean_window_error, std_window_error, window_labels = get_window_metrics(output, sample, labels, window_len)\n",
    "            frame_stats.append([frame_mean, frame_std, frame_labels])\n",
    "            window_stats.append([mean_window_error, std_window_error, window_labels])\n",
    "            '''\n",
    "            if j % 3 == 0:\n",
    "                animate(sample[0, :, :, :, :], output[0, :, :, :, :], frame_mean, dset, start_time)\n",
    "            '''\n",
    "            \n",
    "\n",
    "    return(frame_stats, window_stats)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "list_of_models = ['Thermal_EditFair_RegularLoss2021-06-11-10-23-32', 'ONI_Depth_FilledFair_RegularLoss2021-06-10-14-14-34', 'ONI_IR_EditFair_RegularLoss2021-05-28-11-06-21',\n",
    "                   'IP_EditFair_RegularLoss2021-05-27-23-44-29', 'ZED_Depth_FilledFair_RegularLoss2021-06-11-12-44-52', 'ZED_RGB_EditFair_RegularLoss2021-06-11-13-41-50'] \n",
    "list_of_datasets = ['Thermal_Edit', 'ONI_Depth_Filled', 'ONI_IR_Edit', 'IP_Edit', 'ZED_Depth_Filled', 'ZED_RGB_Filled'] \n",
    "list_of_files = [\"Edits/Thermal\", \"Edits/ONI_Depth\", \"Edits/ONI_IR\", \"Edits/IP\", \"Edits/ZED_Depth\", \"Edits/ZED_RGB\"] \n",
    "\n",
    "dset = \"Edits/ZED_Depth\" #where the orginal data is stored \n",
    "name = \"ZED_Depth_Filled\" # name of the h5py file\n",
    "\n",
    "\n",
    "start_time = str(datetime.datetime.today().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "modality = (name \n",
    "    + 'Fair_'\n",
    "    + 'RegularLoss')\n",
    "print(start_time)\n",
    "filepath = (\n",
    "    \"Models\\\\\"\n",
    "    + modality\n",
    "    + start_time\n",
    ")\n",
    "filepath = 'Models/ZED_RGB_EditFair_RegularLoss2021-06-11-13-41-50'\n",
    "print(filepath)\n",
    "#train_model(filepath)\n",
    "import functions\n",
    "from functions import animate\n",
    "\n",
    "frame_stats, window_stats = foward_pass(filepath)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functions\n",
    "from functions import get_total_performance_metrics\n",
    "from importlib import reload\n",
    "reload(functions)\n",
    "from functions import get_total_performance_metrics\n",
    "np.save(\"Recon_Errors\\\\frame_stats_{}.npy\".format(modality), frame_stats)\n",
    "np.save(\"Recon_Errors\\\\window_stats{}.npy\".format(modality), window_stats)\n",
    "frame_stats2 = np.load(\"Recon_Errors\\\\frame_stats_{}.npy\".format(modality), allow_pickle=True)\n",
    "window_stats2 = np.load(\"Recon_Errors\\\\window_stats{}.npy\".format(modality), allow_pickle=True)\n",
    "get_total_performance_metrics(frame_stats, window_stats, window_len)\n",
    "\n",
    "\n",
    "#get_total_performance_metrics(originals, reconstruced, testing_labels, window_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Fall0', 'Fall1', 'Fall10', 'Fall100', 'Fall101', 'Fall102', 'Fall103', 'Fall104', 'Fall105', 'Fall106', 'Fall107', 'Fall108', 'Fall109', 'Fall11', 'Fall110', 'Fall111', 'Fall112', 'Fall113', 'Fall114', 'Fall115', 'Fall116', 'Fall117', 'Fall118', 'Fall119', 'Fall12', 'Fall120', 'Fall121', 'Fall122', 'Fall123', 'Fall124', 'Fall125', 'Fall126', 'Fall127', 'Fall128', 'Fall129', 'Fall13', 'Fall130', 'Fall131', 'Fall132', 'Fall133', 'Fall134', 'Fall135', 'Fall136', 'Fall137', 'Fall138', 'Fall139', 'Fall14', 'Fall15', 'Fall150', 'Fall151', 'Fall152', 'Fall153', 'Fall154', 'Fall155', 'Fall156', 'Fall157', 'Fall158', 'Fall159', 'Fall16', 'Fall160', 'Fall161', 'Fall162', 'Fall163', 'Fall164', 'Fall165', 'Fall166', 'Fall167', 'Fall168', 'Fall169', 'Fall17', 'Fall170', 'Fall171', 'Fall172', 'Fall173', 'Fall174', 'Fall175', 'Fall176', 'Fall177', 'Fall178', 'Fall179', 'Fall18', 'Fall180', 'Fall181', 'Fall182', 'Fall183', 'Fall184', 'Fall185', 'Fall186', 'Fall187', 'Fall188', 'Fall189', 'Fall19', 'Fall190', 'Fall191', 'Fall192', 'Fall193', 'Fall194', 'Fall195', 'Fall196', 'Fall197', 'Fall198', 'Fall199', 'Fall2', 'Fall20', 'Fall200', 'Fall201', 'Fall202', 'Fall203', 'Fall204', 'Fall205', 'Fall206', 'Fall207', 'Fall208', 'Fall209', 'Fall210', 'Fall211', 'Fall212', 'Fall213', 'Fall214', 'Fall215', 'Fall216', 'Fall217', 'Fall218', 'Fall219', 'Fall23', 'Fall230', 'Fall231', 'Fall232', 'Fall233', 'Fall234', 'Fall235', 'Fall236', 'Fall237', 'Fall238', 'Fall239', 'Fall24', 'Fall25', 'Fall26', 'Fall260', 'Fall261', 'Fall262', 'Fall263', 'Fall264', 'Fall265', 'Fall266', 'Fall267', 'Fall268', 'Fall269', 'Fall27', 'Fall270', 'Fall271', 'Fall272', 'Fall273', 'Fall274', 'Fall275', 'Fall276', 'Fall277', 'Fall278', 'Fall279', 'Fall28', 'Fall280', 'Fall281', 'Fall282', 'Fall283', 'Fall284', 'Fall285', 'Fall286', 'Fall287', 'Fall288', 'Fall289', 'Fall29', 'Fall290', 'Fall291', 'Fall292', 'Fall293', 'Fall294', 'Fall295', 'Fall296', 'Fall297', 'Fall298', 'Fall299', 'Fall3', 'Fall30', 'Fall31', 'Fall32', 'Fall33', 'Fall34', 'Fall35', 'Fall36', 'Fall37', 'Fall38', 'Fall39', 'Fall4', 'Fall40', 'Fall41', 'Fall42', 'Fall43', 'Fall44', 'Fall45', 'Fall46', 'Fall47', 'Fall48', 'Fall49', 'Fall5', 'Fall50', 'Fall51', 'Fall52', 'Fall53', 'Fall54', 'Fall55', 'Fall56', 'Fall57', 'Fall58', 'Fall59', 'Fall6', 'Fall60', 'Fall61', 'Fall62', 'Fall63', 'Fall64', 'Fall65', 'Fall66', 'Fall67', 'Fall68', 'Fall69', 'Fall7', 'Fall70', 'Fall71', 'Fall72', 'Fall73', 'Fall74', 'Fall75', 'Fall76', 'Fall77', 'Fall78', 'Fall79', 'Fall8', 'Fall80', 'Fall81', 'Fall82', 'Fall83', 'Fall84', 'Fall85', 'Fall86', 'Fall87', 'Fall88', 'Fall89', 'Fall9', 'Fall90', 'Fall91', 'Fall92', 'Fall93', 'Fall94', 'Fall95', 'Fall96', 'Fall97', 'Fall98', 'Fall99']\n",
      "['NonFall0', 'NonFall1', 'NonFall10', 'NonFall11', 'NonFall12', 'NonFall13', 'NonFall14', 'NonFall15', 'NonFall16', 'NonFall17', 'NonFall18', 'NonFall19', 'NonFall2', 'NonFall20', 'NonFall21', 'NonFall22', 'NonFall23', 'NonFall24', 'NonFall25', 'NonFall26', 'NonFall27', 'NonFall28', 'NonFall29', 'NonFall3', 'NonFall30', 'NonFall31', 'NonFall32', 'NonFall33', 'NonFall34', 'NonFall35', 'NonFall36', 'NonFall37', 'NonFall38', 'NonFall39', 'NonFall4', 'NonFall40', 'NonFall41', 'NonFall42', 'NonFall43', 'NonFall44', 'NonFall45', 'NonFall46', 'NonFall47', 'NonFall48', 'NonFall49', 'NonFall5', 'NonFall50', 'NonFall51', 'NonFall52', 'NonFall53', 'NonFall54', 'NonFall55', 'NonFall56', 'NonFall57', 'NonFall58', 'NonFall59', 'NonFall6', 'NonFall60', 'NonFall61', 'NonFall62', 'NonFall63', 'NonFall64', 'NonFall65', 'NonFall66', 'NonFall67', 'NonFall68', 'NonFall69', 'NonFall7', 'NonFall70', 'NonFall71', 'NonFall72', 'NonFall73', 'NonFall74', 'NonFall75', 'NonFall76', 'NonFall77', 'NonFall78', 'NonFall79', 'NonFall8', 'NonFall80', 'NonFall81', 'NonFall82', 'NonFall83', 'NonFall84', 'NonFall85', 'NonFall86', 'NonFall87', 'NonFall88', 'NonFall89', 'NonFall9']\n",
      "Skipped Fall102\n",
      "Skipped Fall106\n",
      "Skipped Fall108\n",
      "Skipped Fall110\n",
      "Skipped Fall129\n",
      "Skipped Fall131\n",
      "Skipped Fall150\n",
      "Skipped Fall151\n",
      "Skipped Fall152\n",
      "Skipped Fall153\n",
      "Skipped Fall156\n",
      "Skipped Fall157\n",
      "Skipped Fall158\n",
      "Skipped Fall159\n",
      "Skipped Fall167\n",
      "Skipped Fall170\n",
      "Skipped Fall171\n",
      "Skipped Fall182\n",
      "Skipped Fall187\n",
      "Skipped Fall188\n",
      "Skipped Fall20\n",
      "Skipped Fall211\n",
      "Skipped Fall213\n",
      "Skipped Fall214\n",
      "Skipped Fall23\n",
      "Skipped Fall230\n",
      "Skipped Fall231\n",
      "Skipped Fall232\n",
      "Skipped Fall233\n",
      "Skipped Fall234\n",
      "Skipped Fall235\n",
      "Skipped Fall236\n",
      "Skipped Fall237\n",
      "Skipped Fall238\n",
      "Skipped Fall239\n",
      "Skipped Fall25\n",
      "Skipped Fall26\n",
      "Skipped Fall264\n",
      "Skipped Fall267\n",
      "Skipped Fall28\n",
      "Skipped Fall29\n",
      "Skipped Fall290\n",
      "Skipped Fall291\n",
      "Skipped Fall292\n",
      "Skipped Fall293\n",
      "Skipped Fall294\n",
      "Skipped Fall295\n",
      "Skipped Fall296\n",
      "Skipped Fall297\n",
      "Skipped Fall298\n",
      "Skipped Fall299\n",
      "Skipped Fall34\n",
      "Skipped Fall37\n",
      "Skipped Fall40\n",
      "Skipped Fall41\n",
      "Skipped Fall42\n",
      "Skipped Fall43\n",
      "Skipped Fall44\n",
      "Skipped Fall45\n",
      "Skipped Fall46\n",
      "Skipped Fall47\n",
      "Skipped Fall48\n",
      "Skipped Fall49\n",
      "Skipped Fall61\n",
      "Skipped Fall67\n",
      "Skipped Fall72\n",
      "Skipped Fall74\n",
      "Skipped Fall87\n",
      "Skipped Fall91\n",
      "Skipped Fall95\n",
      "Skipped Fall98\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "Skipped Fall99\n",
      "187\n",
      "187\n",
      "64\n",
      "64\n",
      "['NonFall0', 'NonFall1', 'NonFall10', 'NonFall11', 'NonFall12', 'NonFall13', 'NonFall14', 'NonFall15', 'NonFall16', 'NonFall17', 'NonFall18', 'NonFall19', 'NonFall2', 'NonFall24', 'NonFall27', 'NonFall3', 'NonFall30', 'NonFall31', 'NonFall32', 'NonFall33', 'NonFall35', 'NonFall36', 'NonFall38', 'NonFall39', 'NonFall4', 'NonFall5', 'NonFall50', 'NonFall51', 'NonFall52', 'NonFall53', 'NonFall54', 'NonFall55', 'NonFall56', 'NonFall57', 'NonFall58', 'NonFall59', 'NonFall6', 'NonFall60', 'NonFall62', 'NonFall63', 'NonFall64', 'NonFall66', 'NonFall68', 'NonFall69', 'NonFall7', 'NonFall70', 'NonFall71', 'NonFall73', 'NonFall75', 'NonFall76', 'NonFall77', 'NonFall78', 'NonFall79', 'NonFall8', 'NonFall80', 'NonFall81', 'NonFall82', 'NonFall83', 'NonFall84', 'NonFall85', 'NonFall86', 'NonFall88', 'NonFall89', 'NonFall9']\n",
      "['Fall0', 'Fall1', 'Fall10', 'Fall100', 'Fall101', 'Fall103', 'Fall104', 'Fall105', 'Fall107', 'Fall109', 'Fall11', 'Fall111', 'Fall112', 'Fall113', 'Fall114', 'Fall115', 'Fall116', 'Fall117', 'Fall118', 'Fall119', 'Fall12', 'Fall120', 'Fall121', 'Fall122', 'Fall123', 'Fall124', 'Fall125', 'Fall126', 'Fall127', 'Fall128', 'Fall13', 'Fall130', 'Fall132', 'Fall133', 'Fall134', 'Fall135', 'Fall136', 'Fall137', 'Fall138', 'Fall139', 'Fall14', 'Fall15', 'Fall154', 'Fall155', 'Fall16', 'Fall160', 'Fall161', 'Fall162', 'Fall163', 'Fall164', 'Fall165', 'Fall166', 'Fall168', 'Fall169', 'Fall17', 'Fall172', 'Fall173', 'Fall174', 'Fall175', 'Fall176', 'Fall177', 'Fall178', 'Fall179', 'Fall18', 'Fall180', 'Fall181', 'Fall183', 'Fall184', 'Fall185', 'Fall186', 'Fall189', 'Fall19', 'Fall190', 'Fall191', 'Fall192', 'Fall193', 'Fall194', 'Fall195', 'Fall196', 'Fall197', 'Fall198', 'Fall199', 'Fall2', 'Fall200', 'Fall201', 'Fall202', 'Fall203', 'Fall204', 'Fall205', 'Fall206', 'Fall207', 'Fall208', 'Fall209', 'Fall210', 'Fall212', 'Fall215', 'Fall216', 'Fall217', 'Fall218', 'Fall219', 'Fall24', 'Fall260', 'Fall261', 'Fall262', 'Fall263', 'Fall265', 'Fall266', 'Fall268', 'Fall269', 'Fall27', 'Fall270', 'Fall271', 'Fall272', 'Fall273', 'Fall274', 'Fall275', 'Fall276', 'Fall277', 'Fall278', 'Fall279', 'Fall280', 'Fall281', 'Fall282', 'Fall283', 'Fall284', 'Fall285', 'Fall286', 'Fall287', 'Fall288', 'Fall289', 'Fall3', 'Fall30', 'Fall31', 'Fall32', 'Fall33', 'Fall35', 'Fall36', 'Fall38', 'Fall39', 'Fall4', 'Fall5', 'Fall50', 'Fall51', 'Fall52', 'Fall53', 'Fall54', 'Fall55', 'Fall56', 'Fall57', 'Fall58', 'Fall59', 'Fall6', 'Fall60', 'Fall62', 'Fall63', 'Fall64', 'Fall65', 'Fall66', 'Fall68', 'Fall69', 'Fall7', 'Fall70', 'Fall71', 'Fall73', 'Fall75', 'Fall76', 'Fall77', 'Fall78', 'Fall79', 'Fall8', 'Fall80', 'Fall81', 'Fall82', 'Fall83', 'Fall84', 'Fall85', 'Fall86', 'Fall88', 'Fall89', 'Fall9', 'Fall90', 'Fall92', 'Fall93', 'Fall94', 'Fall96', 'Fall97', 'Fall99']\n",
      "   Participant  Trial  Video  Start  Stop  ToD\n",
      "0            1      1      0    752   780    0\n",
      "2            1      2      1    670   680    0\n",
      "3            1      3      2    624   642    0\n",
      "4            1      4      3    424   500    0\n",
      "5            1      5      4    436   448    0\n",
      "cuda\n",
      "foward pass occuring\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-00a00cd41a9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[0mdset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_of_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"H5Data\\Data_set-{}-imgdim64x64.h5\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[0mfull_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodelpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfair_comparison\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-00a00cd41a9b>\u001b[0m in \u001b[0;36mfull_pipeline\u001b[1;34m(name, dset, filepath, window_len, fair_comparison, path, stride)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe_stats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_stats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     \u001b[0mframe_stats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfoward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     modality = (name \n\u001b[0;32m    175\u001b[0m     \u001b[1;33m+\u001b[0m \u001b[1;34m'Fair_'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-00a00cd41a9b>\u001b[0m in \u001b[0;36mfoward_pass\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[0mframe_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_frame_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m                 \u001b[0mmean_window_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_window_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_window_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mframe_stats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mframe_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_labels\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Stefan\\Documents\\lapsum\\functions.py\u001b[0m in \u001b[0;36mget_frame_metrics\u001b[1;34m(sample, output, labels, window_len)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mrecon_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_data\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mrecon_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# ------- Frame Reconstruction Error ---------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Lets load the H%PY dataset into a pytorch dataset class.Please see \n",
    "# dataset_creator on how to generate the H5PY file. \n",
    "window_len = 8\n",
    "stride = 1\n",
    "fair_comparison = False\n",
    "\n",
    "\n",
    "list_of_models = ['Models/Thermal_EditFair_RegularLoss2021-06-11-10-23-32', 'Models/ONI_Depth_FilledFair_RegularLoss2021-06-10-14-14-34', 'Models/ONI_IR_EditFair_RegularLoss2021-05-28-11-06-21', 'Models/IP_EditFair_RegularLoss2021-05-27-23-44-29', 'Models/ZED_Depth_FilledFair_RegularLoss2021-06-11-12-44-52', 'Models/ZED_RGB_EditFair_RegularLoss2021-06-11-13-41-50'] \n",
    "list_of_datasets = ['Thermal_Edit', 'ONI_Depth_Filled', 'ONI_IR_Edit', 'IP_Edit', 'ZED_Depth_Filled', 'ZED_RGB_Filled'] \n",
    "list_of_files = [\"Edits/Thermal\", \"Edits/ONI_Depth\", \"Edits/ONI_IR\", \"Edits/IP\", \"Edits/ZED_Depth\", \"Edits/ZED_RGB\"] \n",
    "\n",
    "def full_pipeline(name, dset, filepath, window_len, fair_comparison, path, stride):\n",
    "\n",
    "\n",
    "    Test_Dataset, test_dataloader, Train_Dataset, train_dataloader = create_pytorch_dataset(name, dset, path, window_len, fair_comparison, stride)\n",
    "\n",
    "\n",
    "    class Autoencoder(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Autoencoder, self).__init__()\n",
    "            # first layer\n",
    "            self.ec1 = nn.Conv3d(1, 16, (5, 3, 3), stride=1, padding=(2, 1, 1),)\n",
    "            self.em1 = nn.MaxPool3d((1, 2, 2), return_indices=True)\n",
    "            #self.ed1 = nn.Dropout3d(p=0.25)\n",
    "            # second layer\n",
    "            self.ec2 = nn.Conv3d(16, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "            self.em2 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "            #self.ed2 = nn.Dropout3d(p=0.25)\n",
    "            # third layer\n",
    "            self.ec3 = nn.Conv3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "            self.em3 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "            # encoding done, time to decode\n",
    "            self.dc1 = nn.ConvTranspose3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "            self.dm1 = nn.MaxUnpool3d((2, 2, 2))\n",
    "            # inverse of 2nd Conv\n",
    "            self.dc2 = nn.ConvTranspose3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "            self.dm2 = nn.MaxUnpool3d((2, 2, 2))\n",
    "            # inverse of 1st Conv\n",
    "            self.dc3 = nn.ConvTranspose3d(8, 16, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "            self.dm3 = nn.MaxUnpool3d((1, 2, 2))\n",
    "            # final inverse\n",
    "            self.dc4 = nn.ConvTranspose3d(16, 1, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "\n",
    "        def forward(self, x):\n",
    "            # *** start of encoder\n",
    "            x = x.permute(1, 0, 2, 3, 4)  # reorder to have correct dimensions\n",
    "            \n",
    "            # (batch_size, chanels, depth, width, height)\n",
    "            _ec1 = F.relu(self.ec1(x))\n",
    "            _em1, i1 = self.em1(_ec1)\n",
    "\n",
    "\n",
    "            #_em1 = self.ed1(_em1) # dropout layer\n",
    "            # second layer \n",
    "            _ec2 = F.relu(self.ec2(_em1))\n",
    "            _em2, i2 = self.em2(_ec2)\n",
    "            \n",
    "            #_em2 = self.ed2(_em2) # dropout layer\n",
    "            # third layer\n",
    "            _ec3 = F.relu(self.ec3(_em2))\n",
    "            _em3, i3 = self.em3(_ec3)\n",
    "        \n",
    "            # print(\"====== Encoding Done =========\")\n",
    "            # *** encoding done, time to decode\n",
    "            _dc1 = F.relu(self.dc1(_em3))\n",
    "            _dm1 = self.dm1(_dc1, i3, output_size=_em2.size())\n",
    "            \n",
    "            # second layer\n",
    "            _dc2 = F.relu(self.dc2(_dm1))\n",
    "            _dm2 = self.dm2(_dc2, i2)\n",
    "            \n",
    "            # third layer\n",
    "            _dc3 = F.relu(self.dc3(_dm2))\n",
    "            _dm3 = self.dm3(_dc3, i1)\n",
    "            \n",
    "            re_x = torch.tanh(self.dc4(_dm3))\n",
    "            \n",
    "            return re_x\n",
    "        \n",
    "\n",
    "    # Now lets train our model\n",
    "\n",
    "    # prepare for GPU training \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(device)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # and lets set the hyperparameters! \n",
    "\n",
    "    dropout = 0.25\n",
    "    learning_rate = 0.0002\n",
    "    num_epochs = 20\n",
    "    chunk_size = 128\n",
    "    forward_chunk = 8 \n",
    "    forward_chunk_size = 8 # this is smaller due to memory constrains \n",
    "\n",
    "    # select which model - you could load your own or put it in the function above \n",
    "    model = Autoencoder().to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "    def train_model(filepath):\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            val_loss = 0\n",
    "            for i, (sample, labels) in enumerate(train_dataloader):\n",
    "                # ===================forward=====================\n",
    "                sample = sample.to(device, dtype=torch.float)\n",
    "                # split sample into smaller sizes due to GPU memory constraints\n",
    "                chunks = torch.split(sample, chunk_size, dim=1)\n",
    "                for chunk in chunks:\n",
    "                    output = model(chunk)\n",
    "                    output = output.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    model.zero_grad()\n",
    "                    loss = loss_fn(output, chunk)\n",
    "                    # ===================backward====================\n",
    "                    # Getting gradients w.r.t. parameters\n",
    "                    loss.backward()\n",
    "                    # Updating parameters\n",
    "                    optimizer.step()\n",
    "                    # Clear gradients w.r.t. parameters\n",
    "                    optimizer.zero_grad()\n",
    "                    torch.cuda.empty_cache()\n",
    "            # ===================log========================\n",
    "            print(\"epoch [{}/{}], loss:{:.4f}\".format(epoch + 1, num_epochs, loss.item()))\n",
    "            torch.save(model.state_dict(), filepath) # save the model each epoch at location filepath\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "\n",
    "    def foward_pass(path):\n",
    "        model.load_state_dict(torch.load(path)) # load a saved model \n",
    "        model.eval()\n",
    "        frame_stats = [] \n",
    "        window_stats = [] \n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(\"foward pass occuring\")\n",
    "            # just forward pass of model on test dataset\n",
    "            for j, (sample, labels) in enumerate(test_dataloader):\n",
    "                print(j)\n",
    "                # foward pass to get output\n",
    "                torch.cuda.empty_cache()\n",
    "                sample = sample.to(device, dtype=torch.float)\n",
    "                chunks = torch.split(sample, forward_chunk, dim=1)\n",
    "                recon_vid = []\n",
    "                for chunk in chunks:\n",
    "                    output = model(chunk)\n",
    "                    output = output.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    recon_vid.append(output)\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                output = torch.cat(recon_vid, dim=1)\n",
    "                # convert tensors to numpy arrays for easy manipluations\n",
    "                sample = sample.data.cpu().numpy()\n",
    "                output = output.data.cpu().numpy()\n",
    "                labels = labels.data.cpu().numpy()\n",
    "\n",
    "                frame_mean, frame_std, frame_labels = get_frame_metrics(output, sample, labels, window_len)\n",
    "                mean_window_error, std_window_error, window_labels = get_window_metrics(output, sample, labels, window_len)\n",
    "                frame_stats.append([frame_mean, frame_std, frame_labels])\n",
    "                window_stats.append([mean_window_error, std_window_error, window_labels])\n",
    "                '''\n",
    "                if j % 3 == 0:\n",
    "                    animate(sample[0, :, :, :, :], output[0, :, :, :, :], frame_mean, dset, start_time)\n",
    "                '''\n",
    "                \n",
    "\n",
    "        return(frame_stats, window_stats)\n",
    "    \n",
    "    frame_stats, window_stats = foward_pass(filepath)\n",
    "    modality = (name \n",
    "    + 'Fair_'\n",
    "    + 'RegularLoss')\n",
    "\n",
    "\n",
    "    np.save(\"Recon_Errors\\\\frame_stats_{}.npy\".format(modality), frame_stats)\n",
    "    np.save(\"Recon_Errors\\\\window_stats{}.npy\".format(modality), window_stats)\n",
    "    frame_stats2 = np.load(\"Recon_Errors\\\\frame_stats_{}.npy\".format(modality), allow_pickle=True)\n",
    "    window_stats2 = np.load(\"Recon_Errors\\\\window_stats{}.npy\".format(modality), allow_pickle=True)\n",
    "    get_total_performance_metrics(frame_stats, window_stats, window_len)\n",
    "    \n",
    "    return()\n",
    "\n",
    "\n",
    "for i in range(len(list_of_models)):\n",
    "    modelpath = list_of_models[i]\n",
    "    name = list_of_datasets[i]\n",
    "    dset = list_of_files[i]\n",
    "    path = \"H5Data\\Data_set-{}-imgdim64x64.h5\".format(name)\n",
    "    full_pipeline(name, dset, modelpath, window_len, fair_comparison, path, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}