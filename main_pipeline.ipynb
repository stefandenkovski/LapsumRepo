{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook will allow you to run a model on a single prepared dataset. \n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import ffmpeg\n",
    "import pdb\n",
    "from functions import create_pytorch_dataset\n",
    "from functions import get_window_metrics\n",
    "from functions import get_frame_metrics\n",
    "from functions import animate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Fall0', 'Fall1', 'Fall10', 'Fall100', 'Fall101', 'Fall102', 'Fall103', 'Fall104', 'Fall105', 'Fall106', 'Fall107', 'Fall108', 'Fall109', 'Fall11', 'Fall110', 'Fall111', 'Fall112', 'Fall113', 'Fall114', 'Fall115', 'Fall116', 'Fall117', 'Fall118', 'Fall119', 'Fall12', 'Fall120', 'Fall121', 'Fall122', 'Fall123', 'Fall124', 'Fall125', 'Fall126', 'Fall127', 'Fall128', 'Fall129', 'Fall13', 'Fall130', 'Fall131', 'Fall132', 'Fall133', 'Fall134', 'Fall135', 'Fall136', 'Fall137', 'Fall138', 'Fall139', 'Fall14', 'Fall15', 'Fall150', 'Fall151', 'Fall152', 'Fall153', 'Fall154', 'Fall155', 'Fall156', 'Fall157', 'Fall158', 'Fall159', 'Fall16', 'Fall160', 'Fall161', 'Fall162', 'Fall163', 'Fall164', 'Fall165', 'Fall166', 'Fall167', 'Fall168', 'Fall169', 'Fall17', 'Fall170', 'Fall171', 'Fall172', 'Fall173', 'Fall174', 'Fall175', 'Fall176', 'Fall177', 'Fall178', 'Fall179', 'Fall18', 'Fall180', 'Fall181', 'Fall182', 'Fall183', 'Fall184', 'Fall185', 'Fall186', 'Fall187', 'Fall188', 'Fall189', 'Fall19', 'Fall190', 'Fall191', 'Fall192', 'Fall193', 'Fall194', 'Fall195', 'Fall196', 'Fall197', 'Fall198', 'Fall199', 'Fall2', 'Fall20', 'Fall200', 'Fall201', 'Fall202', 'Fall203', 'Fall204', 'Fall205', 'Fall206', 'Fall207', 'Fall208', 'Fall209', 'Fall21', 'Fall210', 'Fall211', 'Fall212', 'Fall213', 'Fall214', 'Fall215', 'Fall216', 'Fall217', 'Fall218', 'Fall219', 'Fall22', 'Fall220', 'Fall221', 'Fall222', 'Fall223', 'Fall224', 'Fall225', 'Fall226', 'Fall227', 'Fall228', 'Fall229', 'Fall23', 'Fall230', 'Fall231', 'Fall232', 'Fall233', 'Fall234', 'Fall235', 'Fall236', 'Fall237', 'Fall238', 'Fall239', 'Fall24', 'Fall240', 'Fall241', 'Fall242', 'Fall243', 'Fall244', 'Fall245', 'Fall246', 'Fall247', 'Fall248', 'Fall249', 'Fall25', 'Fall250', 'Fall251', 'Fall252', 'Fall253', 'Fall254', 'Fall255', 'Fall256', 'Fall257', 'Fall258', 'Fall259', 'Fall26', 'Fall260', 'Fall261', 'Fall262', 'Fall263', 'Fall264', 'Fall265', 'Fall266', 'Fall267', 'Fall268', 'Fall269', 'Fall27', 'Fall270', 'Fall271', 'Fall272', 'Fall273', 'Fall274', 'Fall275', 'Fall276', 'Fall277', 'Fall278', 'Fall279', 'Fall28', 'Fall280', 'Fall281', 'Fall282', 'Fall283', 'Fall284', 'Fall285', 'Fall286', 'Fall287', 'Fall288', 'Fall289', 'Fall29', 'Fall3', 'Fall30', 'Fall31', 'Fall32', 'Fall33', 'Fall34', 'Fall35', 'Fall36', 'Fall37', 'Fall38', 'Fall39', 'Fall4', 'Fall40', 'Fall41', 'Fall42', 'Fall43', 'Fall44', 'Fall45', 'Fall46', 'Fall47', 'Fall48', 'Fall49', 'Fall5', 'Fall50', 'Fall51', 'Fall52', 'Fall53', 'Fall54', 'Fall55', 'Fall56', 'Fall57', 'Fall58', 'Fall59', 'Fall6', 'Fall60', 'Fall61', 'Fall62', 'Fall63', 'Fall64', 'Fall65', 'Fall66', 'Fall67', 'Fall68', 'Fall69', 'Fall7', 'Fall70', 'Fall71', 'Fall72', 'Fall73', 'Fall74', 'Fall75', 'Fall76', 'Fall77', 'Fall78', 'Fall79', 'Fall8', 'Fall80', 'Fall81', 'Fall82', 'Fall83', 'Fall84', 'Fall85', 'Fall86', 'Fall87', 'Fall88', 'Fall89', 'Fall9', 'Fall90', 'Fall91', 'Fall92', 'Fall93', 'Fall94', 'Fall95', 'Fall96', 'Fall97', 'Fall98', 'Fall99']\n",
      "['NonFall0', 'NonFall1', 'NonFall10', 'NonFall11', 'NonFall12', 'NonFall13', 'NonFall14', 'NonFall15', 'NonFall16', 'NonFall17', 'NonFall18', 'NonFall19', 'NonFall2', 'NonFall20', 'NonFall21', 'NonFall22', 'NonFall23', 'NonFall24', 'NonFall25', 'NonFall26', 'NonFall27', 'NonFall28', 'NonFall29', 'NonFall3', 'NonFall30', 'NonFall31', 'NonFall32', 'NonFall33', 'NonFall34', 'NonFall35', 'NonFall36', 'NonFall37', 'NonFall38', 'NonFall39', 'NonFall4', 'NonFall40', 'NonFall41', 'NonFall42', 'NonFall43', 'NonFall44', 'NonFall45', 'NonFall46', 'NonFall47', 'NonFall48', 'NonFall49', 'NonFall5', 'NonFall6', 'NonFall60', 'NonFall61', 'NonFall62', 'NonFall63', 'NonFall64', 'NonFall65', 'NonFall66', 'NonFall67', 'NonFall68', 'NonFall69', 'NonFall7', 'NonFall70', 'NonFall71', 'NonFall72', 'NonFall73', 'NonFall74', 'NonFall75', 'NonFall76', 'NonFall77', 'NonFall78', 'NonFall79', 'NonFall8', 'NonFall80', 'NonFall81', 'NonFall82', 'NonFall83', 'NonFall84', 'NonFall85', 'NonFall86', 'NonFall87', 'NonFall88', 'NonFall89', 'NonFall9', 'NonFall90', 'NonFall91', 'NonFall92', 'NonFall93', 'NonFall94', 'NonFall95', 'NonFall96', 'NonFall97', 'NonFall98', 'NonFall99']\n",
      "280\n",
      "280\n",
      "90\n",
      "90\n",
      "['NonFall0', 'NonFall1', 'NonFall10', 'NonFall11', 'NonFall12', 'NonFall13', 'NonFall14', 'NonFall15', 'NonFall16', 'NonFall17', 'NonFall18', 'NonFall19', 'NonFall2', 'NonFall20', 'NonFall21', 'NonFall22', 'NonFall23', 'NonFall24', 'NonFall25', 'NonFall26', 'NonFall27', 'NonFall28', 'NonFall29', 'NonFall3', 'NonFall30', 'NonFall31', 'NonFall32', 'NonFall33', 'NonFall34', 'NonFall35', 'NonFall36', 'NonFall37', 'NonFall38', 'NonFall39', 'NonFall4', 'NonFall40', 'NonFall41', 'NonFall42', 'NonFall43', 'NonFall44', 'NonFall45', 'NonFall46', 'NonFall47', 'NonFall48', 'NonFall49', 'NonFall5', 'NonFall6', 'NonFall60', 'NonFall61', 'NonFall62', 'NonFall63', 'NonFall64', 'NonFall65', 'NonFall66', 'NonFall67', 'NonFall68', 'NonFall69', 'NonFall7', 'NonFall70', 'NonFall71', 'NonFall72', 'NonFall73', 'NonFall74', 'NonFall75', 'NonFall76', 'NonFall77', 'NonFall78', 'NonFall79', 'NonFall8', 'NonFall80', 'NonFall81', 'NonFall82', 'NonFall83', 'NonFall84', 'NonFall85', 'NonFall86', 'NonFall87', 'NonFall88', 'NonFall89', 'NonFall9', 'NonFall90', 'NonFall91', 'NonFall92', 'NonFall93', 'NonFall94', 'NonFall95', 'NonFall96', 'NonFall97', 'NonFall98', 'NonFall99']\n",
      "['Fall0', 'Fall1', 'Fall10', 'Fall100', 'Fall101', 'Fall102', 'Fall103', 'Fall104', 'Fall105', 'Fall106', 'Fall107', 'Fall108', 'Fall109', 'Fall11', 'Fall110', 'Fall111', 'Fall112', 'Fall113', 'Fall114', 'Fall115', 'Fall116', 'Fall117', 'Fall118', 'Fall119', 'Fall12', 'Fall120', 'Fall121', 'Fall122', 'Fall123', 'Fall124', 'Fall125', 'Fall126', 'Fall127', 'Fall128', 'Fall129', 'Fall13', 'Fall130', 'Fall131', 'Fall132', 'Fall133', 'Fall134', 'Fall135', 'Fall136', 'Fall137', 'Fall138', 'Fall139', 'Fall14', 'Fall15', 'Fall150', 'Fall151', 'Fall152', 'Fall153', 'Fall154', 'Fall155', 'Fall156', 'Fall157', 'Fall158', 'Fall159', 'Fall16', 'Fall160', 'Fall161', 'Fall162', 'Fall163', 'Fall164', 'Fall165', 'Fall166', 'Fall167', 'Fall168', 'Fall169', 'Fall17', 'Fall170', 'Fall171', 'Fall172', 'Fall173', 'Fall174', 'Fall175', 'Fall176', 'Fall177', 'Fall178', 'Fall179', 'Fall18', 'Fall180', 'Fall181', 'Fall182', 'Fall183', 'Fall184', 'Fall185', 'Fall186', 'Fall187', 'Fall188', 'Fall189', 'Fall19', 'Fall190', 'Fall191', 'Fall192', 'Fall193', 'Fall194', 'Fall195', 'Fall196', 'Fall197', 'Fall198', 'Fall199', 'Fall2', 'Fall20', 'Fall200', 'Fall201', 'Fall202', 'Fall203', 'Fall204', 'Fall205', 'Fall206', 'Fall207', 'Fall208', 'Fall209', 'Fall21', 'Fall210', 'Fall211', 'Fall212', 'Fall213', 'Fall214', 'Fall215', 'Fall216', 'Fall217', 'Fall218', 'Fall219', 'Fall22', 'Fall220', 'Fall221', 'Fall222', 'Fall223', 'Fall224', 'Fall225', 'Fall226', 'Fall227', 'Fall228', 'Fall229', 'Fall23', 'Fall230', 'Fall231', 'Fall232', 'Fall233', 'Fall234', 'Fall235', 'Fall236', 'Fall237', 'Fall238', 'Fall239', 'Fall24', 'Fall240', 'Fall241', 'Fall242', 'Fall243', 'Fall244', 'Fall245', 'Fall246', 'Fall247', 'Fall248', 'Fall249', 'Fall25', 'Fall250', 'Fall251', 'Fall252', 'Fall253', 'Fall254', 'Fall255', 'Fall256', 'Fall257', 'Fall258', 'Fall259', 'Fall26', 'Fall260', 'Fall261', 'Fall262', 'Fall263', 'Fall264', 'Fall265', 'Fall266', 'Fall267', 'Fall268', 'Fall269', 'Fall27', 'Fall270', 'Fall271', 'Fall272', 'Fall273', 'Fall274', 'Fall275', 'Fall276', 'Fall277', 'Fall278', 'Fall279', 'Fall28', 'Fall280', 'Fall281', 'Fall282', 'Fall283', 'Fall284', 'Fall285', 'Fall286', 'Fall287', 'Fall288', 'Fall289', 'Fall29', 'Fall3', 'Fall30', 'Fall31', 'Fall32', 'Fall33', 'Fall34', 'Fall35', 'Fall36', 'Fall37', 'Fall38', 'Fall39', 'Fall4', 'Fall40', 'Fall41', 'Fall42', 'Fall43', 'Fall44', 'Fall45', 'Fall46', 'Fall47', 'Fall48', 'Fall49', 'Fall5', 'Fall50', 'Fall51', 'Fall52', 'Fall53', 'Fall54', 'Fall55', 'Fall56', 'Fall57', 'Fall58', 'Fall59', 'Fall6', 'Fall60', 'Fall61', 'Fall62', 'Fall63', 'Fall64', 'Fall65', 'Fall66', 'Fall67', 'Fall68', 'Fall69', 'Fall7', 'Fall70', 'Fall71', 'Fall72', 'Fall73', 'Fall74', 'Fall75', 'Fall76', 'Fall77', 'Fall78', 'Fall79', 'Fall8', 'Fall80', 'Fall81', 'Fall82', 'Fall83', 'Fall84', 'Fall85', 'Fall86', 'Fall87', 'Fall88', 'Fall89', 'Fall9', 'Fall90', 'Fall91', 'Fall92', 'Fall93', 'Fall94', 'Fall95', 'Fall96', 'Fall97', 'Fall98', 'Fall99']\n",
      "   Participant  Trial  Video  Start  Stop  ToD  FallLength\n",
      "0            1      1      0    980   986    0          17\n",
      "2            1      2      1   1110  1119    0          21\n",
      "3            1      3      2    693   700    0          18\n",
      "4            1      4      3    641   654    0          33\n",
      "5            1      5      4    676   687    0          28\n"
     ]
    }
   ],
   "source": [
    "# Lets load the H%PY dataset into a pytorch dataset class.Please see \n",
    "# dataset_creator on how to generate the H5PY file. \n",
    "\n",
    "# Name of the H5PY dataset \n",
    "dset = \"Edits/IP\" #where the orginal data is stored \n",
    "name = \"IP_Edit\" # name of the h5py file\n",
    "path = \"H5Data\\Data_set-{}-imgdim64x64.h5\".format(name) # location of the h5py file\n",
    "# this will also window the data at a set size, and with the set stride \n",
    "\n",
    "window_len = 8\n",
    "stride = 1\n",
    "\n",
    "Test_Dataset, test_dataloader, Train_Dataset, train_dataloader = create_pytorch_dataset(name, dset, path, window_len, stride)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now that we have our datasets prepared, lets write our model \n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # first layer\n",
    "        self.ec1 = nn.Conv3d(1, 16, (5, 3, 3), stride=1, padding=(2, 1, 1),)\n",
    "        self.em1 = nn.MaxPool3d((1, 2, 2), return_indices=True)\n",
    "        self.ed1 = nn.Dropout3d(p=0.25)\n",
    "        # second layer\n",
    "        self.ec2 = nn.Conv3d(16, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.em2 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "        self.ed2 = nn.Dropout3d(p=0.25)\n",
    "        # third layer\n",
    "        self.ec3 = nn.Conv3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.em3 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "        # encoding done, time to decode\n",
    "        self.dc1 = nn.ConvTranspose3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm1 = nn.MaxUnpool3d((2, 2, 2))\n",
    "        # inverse of 2nd Conv\n",
    "        self.dc2 = nn.ConvTranspose3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm2 = nn.MaxUnpool3d((2, 2, 2))\n",
    "        # inverse of 1st Conv\n",
    "        self.dc3 = nn.ConvTranspose3d(8, 16, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm3 = nn.MaxUnpool3d((1, 2, 2))\n",
    "        # final inverse\n",
    "        self.dc4 = nn.ConvTranspose3d(16, 1, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # *** start of encoder\n",
    "        x = x.permute(1, 0, 2, 3, 4)  # reorder to have correct dimensions\n",
    "        # (batch_size, chanels, depth, width, height)\n",
    "        _ec1 = F.relu(self.ec1(x))\n",
    "        _em1, i1 = self.em1(_ec1)\n",
    "        _ec1 = self.ed1(_ec1)\n",
    "        # second layer\n",
    "        _ec2 = F.relu(self.ec2(_em1))\n",
    "        _em2, i2 = self.em2(_ec2)\n",
    "        _em2 = self.ed2(_em2)\n",
    "        # third layer\n",
    "        _ec3 = F.relu(self.ec3(_em2))\n",
    "        _em3, i3 = self.em3(_ec3)\n",
    "        # print(\"====== Encoding Done =========\")\n",
    "        # *** encoding done, time to decode\n",
    "        _dc1 = F.relu(self.dc1(_em3))\n",
    "        _dm1 = self.dm1(_dc1, i3, output_size=_em2.size())\n",
    "        # second layer\n",
    "        _dc2 = F.relu(self.dc2(_dm1))\n",
    "        _dm2 = self.dm2(_dc2, i2)\n",
    "        # third layer\n",
    "        _dc3 = F.relu(self.dc3(_dm2))\n",
    "        _dm3 = self.dm3(_dc3, i1)\n",
    "\n",
    "        re_x = torch.tanh(self.dc4(_dm3))\n",
    "        return re_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Now lets train our model\n",
    "\n",
    "# prepare for GPU training \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# and lets set the hyperparameters! \n",
    "\n",
    "dropout = 0.25\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 20\n",
    "chunk_size = 64\n",
    "forward_chunk = 8 \n",
    "forward_chunk_size = 8 # this is smaller due to memory constrains \n",
    "\n",
    "# select which model - you could load your own or put it in the function above \n",
    "model = Autoencoder().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(filepath):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        val_loss = 0\n",
    "        for i, (sample, labels) in enumerate(train_dataloader):\n",
    "            # ===================forward=====================\n",
    "            sample = sample.to(device, dtype=torch.float)\n",
    "            # split sample into smaller sizes due to GPU memory constraints\n",
    "            chunks = torch.split(sample, chunk_size, dim=1)\n",
    "            for chunk in chunks:\n",
    "                output = model(chunk)\n",
    "                output = output.to(device).permute(1, 0, 2, 3, 4)\n",
    "                model.zero_grad()\n",
    "                loss = loss_fn(output, chunk)\n",
    "                # ===================backward====================\n",
    "                # Getting gradients w.r.t. parameters\n",
    "                loss.backward()\n",
    "                # Updating parameters\n",
    "                optimizer.step()\n",
    "                # Clear gradients w.r.t. parameters\n",
    "                optimizer.zero_grad()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # ===================log========================\n",
    "        print(\"epoch [{}/{}], loss:{:.4f}\".format(epoch + 1, num_epochs, loss.item()))\n",
    "        torch.save(model.state_dict(), filepath) # save the model each epoch at location filepath\n",
    "        \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def foward_pass(path):\n",
    "    model.load_state_dict(torch.load(path)) # load a saved model \n",
    "    model.eval()\n",
    "    frame_stats = [] \n",
    "    window_stats = [] \n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"foward pass occuring\")\n",
    "        # just forward pass of model on test dataset\n",
    "        for j, (sample, labels) in enumerate(test_dataloader):\n",
    "            print(j)\n",
    "            # foward pass to get output\n",
    "            torch.cuda.empty_cache()\n",
    "            sample = sample.to(device, dtype=torch.float)\n",
    "            chunks = torch.split(sample, forward_chunk, dim=1)\n",
    "            recon_vid = []\n",
    "            for chunk in chunks:\n",
    "                output = model(chunk)\n",
    "                output = output.to(device).permute(1, 0, 2, 3, 4)\n",
    "                recon_vid.append(output)\n",
    "                torch.cuda.empty_cache()\n",
    "   \n",
    "            output = torch.cat(recon_vid, dim=1)\n",
    "            # convert tensors to numpy arrays for easy manipluations\n",
    "            sample = sample.data.cpu().numpy()\n",
    "            output = output.data.cpu().numpy()\n",
    "            labels = labels.data.cpu().numpy()\n",
    "\n",
    "            frame_mean, frame_std, frame_labels = get_frame_metrics(output, sample, labels, window_len)\n",
    "            mean_window_error, std_window_error, window_labels = get_window_metrics(output, sample, labels, window_len)\n",
    "            frame_stats.append([frame_mean, frame_std, frame_labels])\n",
    "            window_stats.append([mean_window_error, std_window_error, window_labels])\n",
    "            '''\n",
    "            if j % 50 == 0:\n",
    "                animate(sample[0, :, :, :, :], output[0, :, :, :, :], frame_mean, dset, start_time)\n",
    "            '''\n",
    "            \n",
    "\n",
    "    return(frame_stats, window_stats)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2020-12-03-09-42-45\n",
      "Models\\Edits/IPRegularLoss2020-12-03-09-42-45\n",
      "epoch [1/20], loss:0.0040\n",
      "epoch [2/20], loss:0.0025\n",
      "epoch [3/20], loss:0.0020\n",
      "epoch [4/20], loss:0.0019\n",
      "epoch [5/20], loss:0.0017\n",
      "epoch [6/20], loss:0.0017\n",
      "epoch [7/20], loss:0.0016\n",
      "epoch [8/20], loss:0.0016\n",
      "epoch [9/20], loss:0.0017\n",
      "epoch [10/20], loss:0.0015\n",
      "epoch [11/20], loss:0.0015\n",
      "epoch [12/20], loss:0.0015\n",
      "epoch [13/20], loss:0.0015\n",
      "epoch [14/20], loss:0.0014\n",
      "epoch [15/20], loss:0.0015\n",
      "epoch [16/20], loss:0.0014\n",
      "epoch [17/20], loss:0.0014\n",
      "epoch [18/20], loss:0.0014\n",
      "epoch [19/20], loss:0.0013\n",
      "epoch [20/20], loss:0.0014\n",
      "foward pass occuring\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = str(datetime.datetime.today().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "print(start_time)\n",
    "filepath = (\n",
    "    \"Models\\\\\"\n",
    "    + dset \n",
    "    + 'RegularLoss'\n",
    "    + start_time\n",
    ")\n",
    "#filepath = 'Models\\Edits\\ONI_Depth2020-11-26-10-07-38'\n",
    "print(filepath)\n",
    "train_model(filepath)\n",
    "import functions\n",
    "from functions import animate\n",
    "\n",
    "\n",
    "frame_stats, window_stats = foward_pass(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(280, 5, 8)\n",
      "saving\n"
     ]
    }
   ],
   "source": [
    "import functions\n",
    "from functions import get_total_performance_metrics\n",
    "from importlib import reload\n",
    "reload(functions)\n",
    "from functions import get_total_performance_metrics\n",
    "\n",
    "get_total_performance_metrics(frame_stats, window_stats, window_len)\n",
    "\n",
    "\n",
    "#get_total_performance_metrics(originals, reconstruced, testing_labels, window_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}