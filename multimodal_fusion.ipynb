{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import datetime\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import ffmpeg\n",
    "import pdb\n",
    "from functions import create_pytorch_dataset, create_multimodal_pytorch_dataset\n",
    "from functions import get_window_metrics\n",
    "from functions import get_frame_metrics\n",
    "from functions import animate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Edits/ZED_RGB\n",
      "Edits/ZED_Depth\n",
      "ZED_RGB_Edit\n",
      "ZED_Depth_Edit\n",
      "H5Data\\data_set-ZED_RGB_Edit-imgdim64x64.h5\n",
      "Edits/ZED_RGB\n",
      "ZED_RGB_Edit\n",
      "H5Data\\data_set-ZED_RGB_Edit-imgdim64x64.h5\n",
      "297\n",
      "297\n",
      "99\n",
      "99\n",
      "H5Data\\data_set-ZED_Depth_Edit-imgdim64x64.h5\n",
      "Edits/ZED_Depth\n",
      "ZED_Depth_Edit\n",
      "H5Data\\data_set-ZED_Depth_Edit-imgdim64x64.h5\n",
      "297\n",
      "297\n",
      "99\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "# Lets load the H%PY dataset into a pytorch dataset class.Please see \n",
    "# dataset_creator on how to generate the H5PY file. \n",
    "\n",
    "# Name of the H5PY dataset \n",
    "dset = [ 'Edits/ZED_Depth', \"Edits/ONI_Depth\"] #where the orginal data is stored \n",
    "name = [\"ZED_Depth_Edit\", 'ONI_IR_Depth'] # name of the h5py file\n",
    "path = \"H5Data\\Data_set-{}-imgdim64x64.h5\".format(name) # location of the h5py file\n",
    "# this will also window the data at a set size, and with the set stride \n",
    "\n",
    "window_len = 8\n",
    "stride = 1\n",
    "\n",
    "Test_Dataset, test_dataloader, Train_Dataset, train_dataloader = create_multimodal_pytorch_dataset(name, dset, path, window_len, stride)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # define layers \n",
    "        \n",
    "        # first layer\n",
    "        self.ec1 = nn.Conv3d(1, 16, (5, 3, 3), stride=1, padding=(2, 1, 1),)\n",
    "        self.em1 = nn.MaxPool3d((1, 2, 2), return_indices=True)\n",
    "        #self.ed1 = nn.Dropout3d(p=0.25)\n",
    "        # second layer\n",
    "        self.ec2 = nn.Conv3d(16, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.em2 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "        #self.ed2 = nn.Dropout3d(p=0.25)\n",
    "        # third layer\n",
    "        self.ec3 = nn.Conv3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.em3 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "        # encoding done, time to decode\n",
    "        self.dc1 = nn.ConvTranspose3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm1 = nn.MaxUnpool3d((2, 2, 2))\n",
    "        # inverse of 2nd Conv\n",
    "        self.dc2 = nn.ConvTranspose3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm2 = nn.MaxUnpool3d((2, 2, 2))\n",
    "        # inverse of 1st Conv\n",
    "        self.dc3 = nn.ConvTranspose3d(8, 16, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm3 = nn.MaxUnpool3d((1, 2, 2))\n",
    "        # final inverse\n",
    "        self.dc4 = nn.ConvTranspose3d(16, 1, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "\n",
    "        # first layer\n",
    "        self.ec21 = nn.Conv3d(1, 16, (5, 3, 3), stride=1, padding=(2, 1, 1),)\n",
    "        self.em21 = nn.MaxPool3d((1, 2, 2), return_indices=True)\n",
    "        #self.ed21 = nn.Dropout3d(p=0.25)\n",
    "        # second layer\n",
    "        self.ec22 = nn.Conv3d(16, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.em22 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "        #self.ed22 = nn.Dropout3d(p=0.25)\n",
    "        # third layer\n",
    "        self.ec23 = nn.Conv3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.em23 = nn.MaxPool3d((2, 2, 2), return_indices=True)\n",
    "        # encoding done, time to decode\n",
    "        self.dc21 = nn.ConvTranspose3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm21 = nn.MaxUnpool3d((2, 2, 2))\n",
    "        # inverse of 2nd Conv\n",
    "        self.dc22 = nn.ConvTranspose3d(8, 8, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm22 = nn.MaxUnpool3d((2, 2, 2))\n",
    "        # inverse of 1st Conv\n",
    "        self.dc23 = nn.ConvTranspose3d(8, 16, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "        self.dm23 = nn.MaxUnpool3d((1, 2, 2))\n",
    "        # final inverse\n",
    "        self.dc24 = nn.ConvTranspose3d(16, 1, (5, 3, 3), stride=1, padding=(2, 1, 1))\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        # *** start of encoder\n",
    "        x1 = x1.permute(1, 0, 2, 3, 4)  # reorder to have correct dimensions\n",
    "        # (batch_size, chanels, depth, width, height)\n",
    "        _ec1 = F.relu(self.ec1(x1))\n",
    "        _em1, i1 = self.em1(_ec1)\n",
    "        #_ec1 = self.ed1(_ec1)\n",
    "        # second layer\n",
    "        _ec2 = F.relu(self.ec2(_em1))\n",
    "        _em2, i2 = self.em2(_ec2)\n",
    "        #_em2 = self.ed2(_em2)\n",
    "        # third layer\n",
    "        _ec3 = F.relu(self.ec3(_em2))\n",
    "        _em3, i3 = self.em3(_ec3)\n",
    "\n",
    "        \n",
    "        x2 = x2.permute(1, 0, 2, 3, 4)  # reorder to have correct dimensions\n",
    "        # (batch_size, chanels, depth, width, height)\n",
    "        _ec21 = F.relu(self.ec21(x2))\n",
    "        _em21, i1 = self.em21(_ec21)\n",
    "        #_ec21 = self.ed21(_ec21)\n",
    "        # second layer\n",
    "        _ec22 = F.relu(self.ec22(_em21))\n",
    "        _em22, i2 = self.em22(_ec22)\n",
    "        #_em22 = self.ed22(_em22)\n",
    "        # third layer\n",
    "        _ec23 = F.relu(self.ec23(_em22))\n",
    "        _em23, i3 = self.em23(_ec23)\n",
    "\n",
    "        # combined modalaties here \n",
    "        combo1 = (_em23*0.5) * _em3\n",
    "        combo2 = _em23 * (_em3*0.5)\n",
    "\n",
    "\n",
    "        # print(\"====== Encoding Done =========\")\n",
    "        # *** encoding done, time to decode\n",
    "        _dc1 = F.relu(self.dc1(combo1))\n",
    "        _dm1 = self.dm1(_dc1, i3, output_size=_em2.size())\n",
    "        # second layer\n",
    "        _dc2 = F.relu(self.dc2(_dm1))\n",
    "        _dm2 = self.dm2(_dc2, i2)\n",
    "        # third layer\n",
    "        _dc3 = F.relu(self.dc3(_dm2))\n",
    "        _dm3 = self.dm3(_dc3, i1)\n",
    "        re_x1 = torch.tanh(self.dc24(_dm3))\n",
    "\n",
    "        # *** encoding done, time to decode\n",
    "        _dc21 = F.relu(self.dc21(combo2))\n",
    "        _dm21 = self.dm21(_dc21, i3, output_size = _em22.size())\n",
    "        # second layer\n",
    "        _dc22 = F.relu(self.dc22(_dm21))\n",
    "        _dm22 = self.dm22(_dc22, i2)\n",
    "        # third layer\n",
    "        _dc23 = F.relu(self.dc23(_dm22))\n",
    "        _dm23 = self.dm23(_dc23, i1)\n",
    "\n",
    "        re_x2 = torch.tanh(self.dc24(_dm23))\n",
    "\n",
    "        return re_x1, re_x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Now lets train our model\n",
    "\n",
    "# prepare for GPU training \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# and lets set the hyperparameters! \n",
    "\n",
    "dropout = 0.25\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 15\n",
    "chunk_size = 128\n",
    "forward_chunk = 8 \n",
    "forward_chunk_size = 8 # this is smaller due to memory constrains \n",
    "\n",
    "# select which model - you could load your own or put it in the function above\n",
    "model = Autoencoder().to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(filepath):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        val_loss = 0\n",
    "        for i, sample in enumerate(test_dataloader):\n",
    "            x_modality = []\n",
    "            y_modality = []\n",
    "            for j, modalities in enumerate(sample):\n",
    "                if j == 0:\n",
    "                    x_modality = modalities\n",
    "                if j == 1:\n",
    "                    y_modality = modalities\n",
    "            for modality in range(len(x_modality)):\n",
    "                sample1 = x_modality[0]\n",
    "                sample2 = x_modality[1]\n",
    "                # label = y_modality[modality] # dont need label for training\n",
    "                sample1 = sample1.to(device, dtype=torch.float)\n",
    "                sample2 = sample2.to(device, dtype=torch.float)\n",
    "                if sample1.shape[1] > sample2.shape[1]:\n",
    "                    sample1 = sample1[:, :sample2.shape[1], :, :, :]\n",
    "                if sample1.shape[1] < sample2.shape[1]:\n",
    "                    sample2 = sample2[:, :sample1.shape[1], :, :, :]\n",
    "                # split sample into smaller sizes due to GPU memory constraints\n",
    "                chunks1 = torch.split(sample1, chunk_size, dim=1)\n",
    "                chunks2 = torch.split(sample2, chunk_size, dim=1)\n",
    "                for k in range(len(chunks1)):\n",
    "                    output1, output2 = model(chunks1[k], chunks2[k])\n",
    "                    output1 = output1.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    output2 = output2.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    model.zero_grad()\n",
    "                    loss1 = loss_fn(output1, chunks1[k])\n",
    "                    loss2 = loss_fn(output2, chunks2[k])\n",
    "                    loss = loss1 + loss2\n",
    "                    # ===================backward====================\n",
    "                    # Getting gradients w.r.t. parameters\n",
    "                    loss.backward()\n",
    "                    # Updating parameters\n",
    "                    optimizer.step()\n",
    "                    # Clear gradients w.r.t. parameters\n",
    "                    optimizer.zero_grad()\n",
    "                    torch.cuda.empty_cache()\n",
    "                            \n",
    "        # ===================log========================\n",
    "        print(\"epoch [{}/{}], loss:{:.4f}\".format(epoch + 1, num_epochs, loss.item()))\n",
    "        torch.save(model.state_dict(), filepath) # save the model each epoch at location filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foward_pass(path):\n",
    "    model.load_state_dict(torch.load(path)) # load a saved model \n",
    "    model.eval()\n",
    "    frame_stats1 = [] \n",
    "    window_stats1 = [] \n",
    "    frame_stats2 = [] \n",
    "    window_stats2 = [] \n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"foward pass occuring\")\n",
    "        # just forward pass of model on test dataset\n",
    "        for i, sample in enumerate(test_dataloader):\n",
    "            x_modality = []\n",
    "            y_modality = []\n",
    "            for j, modalities in enumerate(sample):\n",
    "                if j == 0:\n",
    "                    x_modality = modalities\n",
    "                if j == 1:\n",
    "                    y_modality = modalities\n",
    "            for modality in range(len(x_modality)):\n",
    "                sample1 = x_modality[0]\n",
    "                sample2 = x_modality[1]\n",
    "                label1 = y_modality[0] # dont need label for training\n",
    "                torch.cuda.empty_cache()\n",
    "                sample1 = sample1.to(device, dtype=torch.float)\n",
    "                sample2 = sample2.to(device, dtype=torch.float)\n",
    "                if sample1.shape[1] > sample2.shape[1]:\n",
    "                    sample1 = sample1[:, :sample2.shape[1], :, :, :]\n",
    "                if sample1.shape[1] < sample2.shape[1]:\n",
    "                    sample2 = sample2[:, :sample1.shape[1], :, :, :]\n",
    "                # split sample into smaller sizes due to GPU memory constraints\n",
    "                chunks1 = torch.split(sample1, chunk_size, dim=1)\n",
    "                chunks2 = torch.split(sample2, chunk_size, dim=1)\n",
    "                recon_vid1 = []\n",
    "                recon_vid2 = []\n",
    "                for k in range(len(chunks1)):\n",
    "                    output1, output2 = model(chunks1[k], chunks2[k])\n",
    "                    output1 = output1.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    output2 = output2.to(device).permute(1, 0, 2, 3, 4)\n",
    "                    recon_vid1.append(output1)\n",
    "                    recon_vid2.append(output2)\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                output1 = torch.cat(recon_vid1, dim=1)\n",
    "                output2 = torch.cat(recon_vid2, dim=1)\n",
    "                # convert tensors to numpy arrays for easy manipluations\n",
    "                sample = sample1.data.cpu().numpy()\n",
    "                output = output1.data.cpu().numpy()\n",
    "                labels = label1.data.cpu().numpy()\n",
    "\n",
    "                frame_mean, frame_std, frame_labels = get_frame_metrics(output, sample, labels, window_len)\n",
    "                mean_window_error, std_window_error, window_labels = get_window_metrics(output, sample, labels, window_len)\n",
    "                frame_stats1.append([frame_mean, frame_std, frame_labels])\n",
    "                window_stats1.append([mean_window_error, std_window_error, window_labels])\n",
    "\n",
    "                sample = sample2.data.cpu().numpy()\n",
    "                output = output2.data.cpu().numpy()\n",
    "                labels = label1.data.cpu().numpy()\n",
    "\n",
    "                frame_mean, frame_std, frame_labels = get_frame_metrics(output, sample, labels, window_len)\n",
    "                mean_window_error, std_window_error, window_labels = get_window_metrics(output, sample, labels, window_len)\n",
    "                frame_stats2.append([frame_mean, frame_std, frame_labels])\n",
    "                window_stats2.append([mean_window_error, std_window_error, window_labels])\n",
    "\n",
    "\n",
    "                '''\n",
    "                if j % 50 == 0:\n",
    "                    animate(sample[0, :, :, :, :], output[0, :, :, :, :], frame_mean, dset, start_time)\n",
    "                '''\n",
    "                    \n",
    "\n",
    "    return(frame_stats1, window_stats1, frame_stats2, window_stats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2020-12-18-00-23-43\n",
      "epoch [1/15], loss:0.0038\n",
      "epoch [2/15], loss:0.0034\n",
      "epoch [3/15], loss:0.0032\n",
      "epoch [4/15], loss:0.0030\n",
      "epoch [5/15], loss:0.0028\n",
      "epoch [6/15], loss:0.0028\n",
      "epoch [7/15], loss:0.0028\n",
      "epoch [8/15], loss:0.0028\n",
      "epoch [9/15], loss:0.0028\n",
      "epoch [10/15], loss:0.0028\n",
      "epoch [11/15], loss:0.0027\n",
      "epoch [12/15], loss:0.0027\n",
      "epoch [13/15], loss:0.0027\n",
      "epoch [14/15], loss:0.0026\n",
      "epoch [15/15], loss:0.0026\n",
      "foward pass occuring\n"
     ]
    }
   ],
   "source": [
    "start_time = str(datetime.datetime.today().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "print(start_time)\n",
    "filepath = (\n",
    "    \"Models\\\\\"\n",
    "    + 'MultiModalFusion_ZED_RGB_DEPTH'\n",
    "    + start_time\n",
    ")\n",
    "#filepath = 'Models\\MultiModalFusion2020-12-14-15-52-54'\n",
    "#print(filepath)\n",
    "train_model(filepath)\n",
    "import functions\n",
    "from functions import animate\n",
    "\n",
    "\n",
    "frame_stats1, window_stats1, frame_stats2, window_stats2 = foward_pass(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "594\n",
      "(594, 5, 8)\n",
      "saving\n",
      "(594, 5, 8)\n",
      "saving\n"
     ]
    }
   ],
   "source": [
    "import functions\n",
    "from functions import get_total_performance_metrics\n",
    "from importlib import reload\n",
    "reload(functions)\n",
    "from functions import get_total_performance_metrics\n",
    "\n",
    "print(len(frame_stats1))\n",
    "get_total_performance_metrics(frame_stats1, window_stats1, window_len)\n",
    "input(\"Press Enter to continue...\")   \n",
    "get_total_performance_metrics(frame_stats2, window_stats2, window_len)\n",
    "\n",
    "#get_total_performance_metrics(originals, reconstruced, testing_labels, window_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}